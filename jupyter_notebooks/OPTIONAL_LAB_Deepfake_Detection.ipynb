{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa613a0-dfe3-4240-9b26-f411c1394da6",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/RiverGumSecurity/AILabs/blob/main/Lab02_Deepfakes/deepfake_audio_detection.ipynb\" target=\"_new\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3258fb4-3f3f-42b9-a669-f3248fd2a26d",
   "metadata": {},
   "source": [
    "# Audio Deepfake Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3848e3-99e4-4eb6-9aa0-8a96b1d6ca9b",
   "metadata": {},
   "source": [
    "As AI-generated audio becomes increasingly realistic, telling it apart from genuine human speech is becoming more difficult. Advanced text-to-speech (TTS) technologies such as F5-TTS, ElevenLabs, and other neural voice synthesis tools can now produce speech that sounds natural and convincing to most listeners. This raises several concerns:\n",
    "\n",
    "- Security: Voice-based authentication methods may be vulnerable\n",
    "\n",
    "- Misinformation: Fake audio recordings could mislead the public\n",
    "\n",
    "- Trust: Itâ€™s harder to verify the authenticity of audio content\n",
    "\n",
    "- Legal: Challenges may arise in validating audio evidence and testimony\n",
    "\n",
    "This notebook explores methods for detecting synthetic speech and presents a system designed to help identify deepfake audio with a high degree of accuracy. It is designed to analyze a single audio files in common formats (WAV, MP3, FLAC, etc.)\n",
    "\n",
    "- Audio Requirements:\n",
    "\n",
    "  - Minimum 3 seconds of speech for reliable detection\n",
    "  - Sample rates automatically normalized to 16kHz\n",
    "  - Works best with clear speech, minimal background noise\n",
    "\n",
    "- No Training Data Required: Uses pre-trained models, so you only need the audio file you want to verify\n",
    "\n",
    "The detection methodology combines multiple complementary techniques:\n",
    "\n",
    "1. Signal Processing Analysis\n",
    "\n",
    "- Extract 50+ acoustic features (MFCCs, spectral features, temporal patterns)\n",
    "- Analyze phase coherence and formant stability\n",
    "- Examine micro-temporal artifacts common in synthetic speech\n",
    "\n",
    "\n",
    "2. Deep Learning Models\n",
    "\n",
    "- Primary: MelodyMachine's dedicated deepfake detection model (50% weight)\n",
    "- Secondary: Wav2Vec2 for audio feature extraction\n",
    "- Ensemble approach for robust detection\n",
    "\n",
    "\n",
    "3. Visual Analysis\n",
    "\n",
    "- Generate spectrograms and mel-spectrograms\n",
    "- Plot temporal and spectral features over time\n",
    "- Visualize phase patterns and pitch contours\n",
    "\n",
    "\n",
    "4. Comprehensive Scoring\n",
    "\n",
    "- Weighted ensemble of all indicators\n",
    "- Component-wise breakdown for interpretability\n",
    "- Confidence levels and risk assessment\n",
    "\n",
    "\n",
    "\n",
    "Key Technologies\n",
    "\n",
    "- Core Libraries: `librosa` for audio processing, `torch` and `transformers` for ML models\n",
    "- Visualization: `matplotlib` and `seaborn` for detailed plots\n",
    "- Signal Processing: `scipy` for advanced audio analysis\n",
    "- Pre-trained Models: HuggingFace transformers ecosystem\n",
    "\n",
    "\n",
    "\n",
    "Clear Detection Result: A score from 0-1 indicating deepfake likelihood\n",
    "\n",
    "- 0.0-0.4: Low risk (likely genuine)\n",
    "- 0.4-0.7: Medium risk (suspicious)\n",
    "- 0.7-1.0: High risk (likely deepfake)\n",
    "\n",
    "\n",
    "Visual Evidence: Multiple plots showing:\n",
    "\n",
    "- Waveform and spectrograms with potential artifacts highlighted\n",
    "- Feature evolution over time\n",
    "- Component scores breakdown\n",
    "\n",
    "\n",
    "Detailed Analysis:\n",
    "\n",
    "- Which specific features triggered detection\n",
    "- Model predictions with confidence scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737844f8-a1ac-4a2d-92b3-4a286a74e7b7",
   "metadata": {},
   "source": [
    "## FFMEG Requirement\n",
    "\n",
    "Note that you will need to install ffmpeg on your system for the model based detection to work. The notebook will function without it, however the strength of the overall detection score will not be as robust withouth it.\n",
    "\n",
    "### Mac OS\n",
    "\n",
    "`brew install ffmpeg`\n",
    "\n",
    "### Ubuntu/Debian/WSL\n",
    "\n",
    "`sudo apt update`\n",
    "`sudo apt install ffmpeg`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9fbb45-7b40-480e-a0bb-89c87fa8594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are having dependency issues uncomment the loine below and run this cell\n",
    "#!pip install torch torchaudio transformers librosa matplotlib seaborn numpy scipy pandas scikit-learn audioread soundfile tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78faeac3-9aa4-4f2a-9253-d0dd952d741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.stats import kurtosis, skew\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoProcessor, AutoModelForAudioClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2b089-364c-42ca-aae4-512826ce0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Load audio file and resample to target sampling rate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try loading with librosa first\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        if sr != target_sr:\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "            sr = target_sr\n",
    "    except:\n",
    "        # Fallback to torchaudio\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if sr != target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(sr, target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "            sr = target_sr\n",
    "        audio = waveform.numpy().squeeze()\n",
    "    \n",
    "    return audio, sr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e382ff-61d3-4f02-bd6f-f31bc5639549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "# Replace with your file path, or uncomment lines below to process provided data\n",
    "\n",
    "audio_file_path = \"deepfake_audio/derek_fake_cosmo_quote.wav\"  \n",
    "#audio_file_path = \"deepfake_audio/joff_fake_cosmo_quote.wav\"  \n",
    "#audio_file_path = \"source_audio/derek_banks_threat_hunting_clip.wav\"  \n",
    "#audio_file_path = \"source_audio/joff_thyer_python_mutate_slice_list.wav\"  \n",
    "\n",
    "audio_data, sample_rate = load_audio(audio_file_path)\n",
    "\n",
    "print(f\"Audio loaded successfully!\")\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Duration: {len(audio_data)/sample_rate:.2f} seconds\")\n",
    "print(f\"Shape: {audio_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bdb9f6-66fc-44d6-8863-85e3f08373c2",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Feature extraction is the foundation of our deepfake detection system. In this section, we extract over 50 distinct acoustic features from the audio signal that have been shown to differ between genuine human speech and AI-generated audio. These features capture subtle characteristics that may not be audible to humans but are statistically significant for detection.\n",
    "\n",
    "### Why Feature Extraction Matters\n",
    "Modern deepfake audio can fool human listeners, but synthetic speech often contains telltale patterns in:\n",
    "\n",
    "- Frequency distribution: How energy is spread across different frequencies\n",
    "- Temporal dynamics: How the signal changes over time\n",
    "- Voice characteristics: Subtle differences in timbre and resonance\n",
    "- Micro-artifacts: Tiny inconsistencies at millisecond scales\n",
    "\n",
    "Feature Categories\n",
    "1. Spectral Features (Frequency Domain)\n",
    "These features analyze the frequency content of the audio:\n",
    "\n",
    "- Spectral Centroid: The \"center of mass\" of the spectrum, indicating brightness\n",
    "\n",
    "    - Real speech shows natural variation\n",
    "    - Synthetic speech may be unnaturally consistent\n",
    "\n",
    "- Spectral Rolloff: Frequency below which 85% of energy is concentrated\n",
    "\n",
    "    - Helps identify unusual frequency distributions\n",
    "    - Can reveal synthesis artifacts in high frequencies\n",
    "\n",
    "- Zero Crossing Rate (ZCR): How often the signal crosses zero\n",
    "\n",
    "    - Indicates voiced vs. unvoiced segments\n",
    "    - Synthetic speech may have unnatural ZCR patterns\n",
    "\n",
    "- MFCCs (Mel-frequency Cepstral Coefficients): 20 coefficients capturing voice timbre\n",
    "\n",
    "    - Industry standard for voice recognition\n",
    "    - Higher coefficients (10-20) particularly sensitive to synthesis\n",
    "\n",
    "- Spectral Contrast: Difference between peaks and valleys in spectrum\n",
    "\n",
    "    - Real speech has natural harmonic structure\n",
    "    - Deepfakes may show altered contrast patterns\n",
    "\n",
    "\n",
    "- Chroma Features: Pitch class information\n",
    "\n",
    "    - Captures tonal characteristics\n",
    "    - Can reveal unnatural pitch stability\n",
    "\n",
    "2. Temporal Features (Time Domain)\n",
    "These features analyze how the signal evolves over time:\n",
    "\n",
    "- Statistical Moments: Mean, variance, kurtosis, skewness\n",
    "\n",
    "    - Capture the overall distribution of the signal\n",
    "    - Synthetic speech may have different statistical properties\n",
    "\n",
    "- Energy Patterns: Frame-by-frame energy analysis\n",
    "\n",
    "    - Real speech has natural energy variations\n",
    "    - Too consistent or erratic energy suggests synthesis\n",
    "\n",
    "- Silence Ratio: Proportion of quiet moments\n",
    "\n",
    "    - Natural speech typically has 5-20% silence\n",
    "    - Synthetic speech often has abnormal pause patterns\n",
    "\n",
    "\n",
    "3. Advanced Deepfake-Specific Features\n",
    "These features specifically target modern TTS artifacts:\n",
    "\n",
    "- Micro-temporal Variance: Frame differences at 5ms scales\n",
    "\n",
    "    - F5-TTS and similar systems may be too consistent\n",
    "    - Captures unnaturally smooth transitions\n",
    "\n",
    "- Spectral Flux: Rate of spectral change\n",
    "\n",
    "    - Real speech has natural spectral evolution\n",
    "    - Synthetic speech may change too smoothly or abruptly\n",
    "\n",
    "- Phase Coherence: Consistency of phase relationships\n",
    "\n",
    "    - Natural speech has moderate phase variation\n",
    "    - High coherence (>0.9) suggests synthesis\n",
    "\n",
    "- MFCC Dynamics: First and second derivatives of MFCCs\n",
    "\n",
    "    - Captures how voice characteristics evolve\n",
    "    - Synthetic speech often has unnatural dynamics\n",
    "\n",
    "- Formant Bandwidth: Spread of vocal tract resonances\n",
    "\n",
    "    - Real speech shows natural variation\n",
    "    - TTS may have overly stable formants\n",
    "\n",
    "- High-Frequency Ratio: Energy above 4kHz relative to total\n",
    "\n",
    "    - Some TTS systems struggle with high-frequency detail\n",
    "    - Can reveal synthesis limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd99c2-209e-4ed2-8095-68f32c3f768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectral_features(audio, sr):\n",
    "    \"\"\"\n",
    "    Extract various spectral features that can help identify deepfakes\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Spectral centroid - brightness indicator\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]\n",
    "    features['spectral_centroid_mean'] = np.mean(spectral_centroids)\n",
    "    features['spectral_centroid_std'] = np.std(spectral_centroids)\n",
    "    \n",
    "    # Spectral rolloff - frequency below which 85% of energy is concentrated\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]\n",
    "    features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
    "    features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
    "    \n",
    "    # Zero crossing rate - roughness indicator\n",
    "    zcr = librosa.feature.zero_crossing_rate(audio)[0]\n",
    "    features['zcr_mean'] = np.mean(zcr)\n",
    "    features['zcr_std'] = np.std(zcr)\n",
    "    \n",
    "    # MFCC statistics\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)\n",
    "    for i in range(20):\n",
    "        features[f'mfcc_{i}_mean'] = np.mean(mfccs[i])\n",
    "        features[f'mfcc_{i}_std'] = np.std(mfccs[i])\n",
    "    \n",
    "    # Spectral contrast\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
    "    for i in range(spectral_contrast.shape[0]):\n",
    "        features[f'spectral_contrast_{i}_mean'] = np.mean(spectral_contrast[i])\n",
    "    \n",
    "    # Chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    features['chroma_mean'] = np.mean(chroma)\n",
    "    features['chroma_std'] = np.std(chroma)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_temporal_features(audio, sr):\n",
    "    \"\"\"\n",
    "    Extract temporal domain features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    features['mean'] = np.mean(audio)\n",
    "    features['std'] = np.std(audio)\n",
    "    features['max'] = np.max(np.abs(audio))\n",
    "    features['kurtosis'] = kurtosis(audio)\n",
    "    features['skewness'] = skew(audio)\n",
    "    \n",
    "    # Energy features\n",
    "    frame_length = int(0.025 * sr)  # 25ms frames\n",
    "    hop_length = int(0.010 * sr)    # 10ms hop\n",
    "    \n",
    "    energy = np.array([\n",
    "        np.sum(audio[i:i+frame_length]**2)\n",
    "        for i in range(0, len(audio)-frame_length, hop_length)\n",
    "    ])\n",
    "    \n",
    "    features['energy_mean'] = np.mean(energy)\n",
    "    features['energy_std'] = np.std(energy)\n",
    "    features['energy_skew'] = skew(energy)\n",
    "    \n",
    "    # Silence ratio\n",
    "    silence_threshold = 0.01 * np.max(np.abs(audio))\n",
    "    silence_frames = np.sum(energy < silence_threshold)\n",
    "    features['silence_ratio'] = silence_frames / len(energy)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_advanced_deepfake_features(audio, sr):\n",
    "    \"\"\"\n",
    "    Extract features specifically designed to detect modern TTS systems like F5-TTS\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # 1. Micro-temporal artifacts\n",
    "    # F5-TTS might have subtle frame-to-frame artifacts\n",
    "    frame_length = int(0.005 * sr)  # 5ms frames\n",
    "    hop_length = int(0.0025 * sr)   # 2.5ms hop\n",
    "    \n",
    "    frame_diffs = []\n",
    "    for i in range(0, len(audio) - frame_length, hop_length):\n",
    "        frame1 = audio[i:i + frame_length]\n",
    "        frame2 = audio[i + hop_length:i + hop_length + frame_length]\n",
    "        if len(frame2) == frame_length:\n",
    "            diff = np.mean(np.abs(frame1 - frame2))\n",
    "            frame_diffs.append(diff)\n",
    "    \n",
    "    if frame_diffs:\n",
    "        features['micro_temporal_variance'] = np.var(frame_diffs)\n",
    "        features['micro_temporal_mean'] = np.mean(frame_diffs)\n",
    "    \n",
    "    # 2. Spectral flux at different scales\n",
    "    # Synthetic speech often has different spectral evolution patterns\n",
    "    stft = librosa.stft(audio, n_fft=2048, hop_length=512)\n",
    "    magnitude = np.abs(stft)\n",
    "    \n",
    "    # Calculate spectral flux\n",
    "    spectral_flux = np.sum(np.diff(magnitude, axis=1)**2, axis=0)\n",
    "    features['spectral_flux_mean'] = np.mean(spectral_flux)\n",
    "    features['spectral_flux_std'] = np.std(spectral_flux)\n",
    "    \n",
    "    # 3. Phase coherence analysis\n",
    "    phase = np.angle(stft)\n",
    "    phase_diff = np.diff(np.unwrap(phase, axis=1), axis=1)\n",
    "    \n",
    "    # Calculate phase coherence across frequency bands\n",
    "    phase_coherence = []\n",
    "    for i in range(0, phase_diff.shape[0] - 10, 10):\n",
    "        band_coherence = np.corrcoef(phase_diff[i], phase_diff[i+5])[0, 1]\n",
    "        if not np.isnan(band_coherence):\n",
    "            phase_coherence.append(band_coherence)\n",
    "    \n",
    "    if phase_coherence:\n",
    "        features['phase_coherence_mean'] = np.mean(phase_coherence)\n",
    "        features['phase_coherence_std'] = np.std(phase_coherence)\n",
    "    \n",
    "    # 4. Mel-frequency cepstral coefficient deltas\n",
    "    # TTS systems might have less natural MFCC evolution\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mfcc_delta = librosa.feature.delta(mfccs)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "    \n",
    "    features['mfcc_delta_variance'] = np.mean(np.var(mfcc_delta, axis=1))\n",
    "    features['mfcc_delta2_variance'] = np.mean(np.var(mfcc_delta2, axis=1))\n",
    "    \n",
    "    # 5. Formant bandwidth analysis (simplified)\n",
    "    # Real speech has natural formant bandwidth variations\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "    features['bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
    "    features['bandwidth_std'] = np.std(spectral_bandwidth)\n",
    "    features['bandwidth_cv'] = features['bandwidth_std'] / (features['bandwidth_mean'] + 1e-6)\n",
    "    \n",
    "    # 6. High-frequency detail analysis\n",
    "    # Some TTS systems have artifacts in high frequencies\n",
    "    high_freq_threshold = sr // 4  # Above quarter Nyquist\n",
    "    high_freq_energy = np.mean(magnitude[high_freq_threshold:, :])\n",
    "    total_energy = np.mean(magnitude)\n",
    "    features['high_freq_ratio'] = high_freq_energy / (total_energy + 1e-6)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features\n",
    "spectral_features = extract_spectral_features(audio_data, sample_rate)\n",
    "temporal_features = extract_temporal_features(audio_data, sample_rate)\n",
    "all_features = {**spectral_features, **temporal_features}\n",
    "\n",
    "# Extract advanced deepfake detection features\n",
    "print(\"\\nExtracting advanced deepfake detection features...\")\n",
    "advanced_features = extract_advanced_deepfake_features(audio_data, sample_rate)\n",
    "all_features.update(advanced_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2e2f0-873a-43fe-ba94-9490a985a5ea",
   "metadata": {},
   "source": [
    "## Visualization Functions\n",
    "This section creates various plots to visually inspect the audio for deepfake artifacts.\n",
    "Visualizations Included:\n",
    "\n",
    "- Waveform: Shows amplitude over time - can reveal unnatural patterns\n",
    "- Spectrogram: Frequency content over time - synthesis artifacts often visible as unusual patterns\n",
    "- Mel Spectrogram: Perceptually-scaled frequency representation - better for spotting voice artifacts\n",
    "- Spectral Features Over Time: Shows how brightness, energy, etc. evolve\n",
    "- MFCC Analysis: Visualizes voice timbre characteristics and their statistics\n",
    "\n",
    "What to Look For:\n",
    "\n",
    "- Too Perfect: Overly smooth or regular patterns suggest synthesis\n",
    "- Artifacts: Unusual lines, gaps, or patterns in spectrograms\n",
    "- Consistency: Unnaturally consistent energy or spectral characteristics\n",
    "- Transitions: Abrupt changes or overly smooth transitions between sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a716c-7b4e-45b9-9a59-b5679cdc7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform_and_spectrogram(audio, sr, title=\"Audio Analysis\"):\n",
    "    \"\"\"\n",
    "    Plot waveform and spectrogram\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Waveform\n",
    "    time = np.arange(len(audio)) / sr\n",
    "    axes[0].plot(time, audio, alpha=0.8)\n",
    "    axes[0].set_title(f'{title} - Waveform')\n",
    "    axes[0].set_xlabel('Time (s)')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[1])\n",
    "    axes[1].set_title(f'{title} - Spectrogram')\n",
    "    fig.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_mel_spectrogram(audio, sr, title=\"Mel Spectrogram\"):\n",
    "    \"\"\"\n",
    "    Plot mel spectrogram - often reveals artifacts in deepfakes\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Compute mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # Plot\n",
    "    img = librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(img, format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_spectral_features(audio, sr):\n",
    "    \"\"\"\n",
    "    Plot various spectral features over time\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "    \n",
    "    # Frame parameters\n",
    "    frame_length = 2048\n",
    "    hop_length = 512\n",
    "    \n",
    "    # Spectral centroid\n",
    "    cent = librosa.feature.spectral_centroid(y=audio, sr=sr, hop_length=hop_length)[0]\n",
    "    frames = range(len(cent))\n",
    "    t = librosa.frames_to_time(frames, sr=sr, hop_length=hop_length)\n",
    "    axes[0].plot(t, cent, label='Spectral Centroid', color='blue', alpha=0.8)\n",
    "    axes[0].set_ylabel('Hz')\n",
    "    axes[0].set_title('Spectral Centroid (Brightness)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Spectral rolloff\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr, hop_length=hop_length)[0]\n",
    "    axes[1].plot(t, rolloff, label='Spectral Rolloff', color='green', alpha=0.8)\n",
    "    axes[1].set_ylabel('Hz')\n",
    "    axes[1].set_title('Spectral Rolloff')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Zero crossing rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(audio, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "    axes[2].plot(t, zcr, label='ZCR', color='red', alpha=0.8)\n",
    "    axes[2].set_ylabel('Rate')\n",
    "    axes[2].set_title('Zero Crossing Rate')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # RMS Energy\n",
    "    rms = librosa.feature.rms(y=audio, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "    axes[3].plot(t, rms, label='RMS Energy', color='purple', alpha=0.8)\n",
    "    axes[3].set_xlabel('Time (s)')\n",
    "    axes[3].set_ylabel('Energy')\n",
    "    axes[3].set_title('RMS Energy')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_mfcc_analysis(audio, sr):\n",
    "    \"\"\"\n",
    "    Plot MFCC coefficients - useful for detecting synthesis artifacts\n",
    "    \"\"\"\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    img = librosa.display.specshow(mfccs, sr=sr, x_axis='time')\n",
    "    plt.colorbar(img)\n",
    "    plt.title('MFCC Coefficients')\n",
    "    plt.ylabel('MFCC Coefficient')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot MFCC statistics\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Mean values\n",
    "    mfcc_means = np.mean(mfccs, axis=1)\n",
    "    axes[0].bar(range(20), mfcc_means)\n",
    "    axes[0].set_title('Mean MFCC Values')\n",
    "    axes[0].set_xlabel('MFCC Coefficient')\n",
    "    axes[0].set_ylabel('Mean Value')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Standard deviations\n",
    "    mfcc_stds = np.std(mfccs, axis=1)\n",
    "    axes[1].bar(range(20), mfcc_stds, color='orange')\n",
    "    axes[1].set_title('MFCC Standard Deviations')\n",
    "    axes[1].set_xlabel('MFCC Coefficient')\n",
    "    axes[1].set_ylabel('Std Dev')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6590c-e939-461a-a28d-4b469cf4457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the audio\n",
    "plot_waveform_and_spectrogram(audio_data, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf6008-da53-4a08-9ac7-4054f9e2a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mel_spectrogram(audio_data, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb58dcf-285e-4695-ba39-a9e1aa12cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectral_features(audio_data, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7343de2a-cccf-48f6-986b-ffbb3828e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mfcc_analysis(audio_data, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a763a-b3d4-42e8-bd47-ccf3d1bc1406",
   "metadata": {},
   "source": [
    "## Advanced Analysis for Deepfake Detection\n",
    "This section performs specialized analyses targeting common deepfake artifacts.\n",
    "\n",
    "Phase Consistency Analysis:\n",
    "Phase relationships in real speech are naturally variable due to the complex interaction of vocal tract components. Synthetic speech often has unnaturally consistent phase relationships.\n",
    "\n",
    "What it analyzes:\n",
    "\n",
    "- Phase Derivative Distribution: How phase changes between frequency bins\n",
    "- Phase Coherence Over Time: Consistency of phase relationships\n",
    "- Expected patterns: Real speech shows moderate variability; too much consistency suggests synthesis\n",
    "\n",
    "Formant and Pitch Analysis:\n",
    "\n",
    "- Formants are resonant frequencies of the vocal tract. Real speech has natural variations in formant frequencies and pitch.\n",
    "\n",
    "What it analyzes:\n",
    "\n",
    "- Formant Tracking: F1 and F2 frequencies over time using Linear Predictive Coding (LPC)\n",
    "- Formant Stability: Standard deviation of formant frequencies\n",
    "- Pitch Contour: Fundamental frequency variations\n",
    "- Harmonic-to-Noise Ratio: Balance between harmonic and noise components\n",
    "\n",
    "Red Flags:\n",
    "\n",
    "- Formants that are too stable (low standard deviation)\n",
    "- Unnaturally smooth pitch contours\n",
    "- Very high HNR values (too periodic/perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8f0bf-883f-4220-ae89-6537c10760d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_phase_consistency(audio, sr):\n",
    "    \"\"\"\n",
    "    Analyze phase consistency - deepfakes often have phase artifacts\n",
    "    \"\"\"\n",
    "    # Compute STFT\n",
    "    D = librosa.stft(audio)\n",
    "    magnitude = np.abs(D)\n",
    "    phase = np.angle(D)\n",
    "    \n",
    "    # Compute phase derivative (instantaneous frequency)\n",
    "    phase_diff = np.diff(np.unwrap(phase, axis=1), axis=1)\n",
    "    \n",
    "    # Plot phase statistics\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Phase derivative histogram\n",
    "    axes[0].hist(phase_diff.flatten(), bins=100, alpha=0.7, density=True)\n",
    "    axes[0].set_title('Phase Derivative Distribution')\n",
    "    axes[0].set_xlabel('Phase Derivative')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Phase coherence over time\n",
    "    phase_coherence = np.std(phase_diff, axis=0)\n",
    "    time_frames = librosa.frames_to_time(range(len(phase_coherence)), sr=sr)\n",
    "    axes[1].plot(time_frames, phase_coherence, alpha=0.8)\n",
    "    axes[1].set_title('Phase Coherence Over Time')\n",
    "    axes[1].set_xlabel('Time (s)')\n",
    "    axes[1].set_ylabel('Phase Coherence')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return phase_diff\n",
    "\n",
    "def analyze_formant_consistency(audio, sr):\n",
    "    \"\"\"\n",
    "    Analyze formant frequencies - synthetic speech often has unnatural formants\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Estimate formants using LPC\n",
    "        from scipy.signal import lfilter\n",
    "        from scipy.signal.windows import hamming\n",
    "        \n",
    "        # Preprocess\n",
    "        audio_filtered = librosa.effects.preemphasis(audio)\n",
    "        \n",
    "        # Window the signal\n",
    "        frame_length = int(0.025 * sr)\n",
    "        hop_length = int(0.010 * sr)\n",
    "        \n",
    "        formants_over_time = []\n",
    "        \n",
    "        for i in range(0, len(audio_filtered) - frame_length, hop_length):\n",
    "            frame = audio_filtered[i:i + frame_length] * hamming(frame_length)\n",
    "            \n",
    "            try:\n",
    "                # LPC analysis\n",
    "                a = librosa.lpc(frame, order=16)\n",
    "                \n",
    "                # Find formants from LPC coefficients\n",
    "                roots = np.roots(a)\n",
    "                roots = roots[np.imag(roots) >= 0]\n",
    "                \n",
    "                # Convert to frequencies\n",
    "                angles = np.angle(roots)\n",
    "                freqs = sorted(angles * sr / (2 * np.pi))\n",
    "                \n",
    "                # Keep only reasonable formant frequencies\n",
    "                formants = [f for f in freqs if 200 < f < 5000][:4]\n",
    "                \n",
    "                if len(formants) >= 2:\n",
    "                    formants_over_time.append(formants[:2])  # F1 and F2\n",
    "            except:\n",
    "                # Skip problematic frames\n",
    "                continue\n",
    "        \n",
    "        if formants_over_time:\n",
    "            formants_array = np.array(formants_over_time)\n",
    "            \n",
    "            # Plot formant tracks\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            time_points = np.arange(len(formants_array)) * hop_length / sr\n",
    "            \n",
    "            plt.scatter(time_points, formants_array[:, 0], alpha=0.6, s=10, label='F1')\n",
    "            plt.scatter(time_points, formants_array[:, 1], alpha=0.6, s=10, label='F2')\n",
    "            \n",
    "            plt.title('Formant Tracking (F1 and F2)')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Frequency (Hz)')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "            \n",
    "            # Formant stability analysis\n",
    "            f1_std = np.std(formants_array[:, 0])\n",
    "            f2_std = np.std(formants_array[:, 1])\n",
    "            print(f\"F1 Standard Deviation: {f1_std:.2f} Hz\")\n",
    "            print(f\"F2 Standard Deviation: {f2_std:.2f} Hz\")\n",
    "        else:\n",
    "            print(\"Could not extract sufficient formant data\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Formant analysis skipped due to error: {e}\")\n",
    "        print(\"This may occur with short audio clips or non-speech audio\")\n",
    "\n",
    "def analyze_pitch_and_harmonics(audio, sr):\n",
    "    \"\"\"\n",
    "    Analyze pitch contour and harmonic structure - alternative to formant analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Method 1: Fundamental frequency using autocorrelation\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Compute short-time autocorrelation\n",
    "        frame_length = int(0.025 * sr)\n",
    "        hop_length = int(0.010 * sr)\n",
    "        \n",
    "        f0_values = []\n",
    "        times = []\n",
    "        \n",
    "        for i in range(0, len(audio) - frame_length, hop_length):\n",
    "            frame = audio[i:i + frame_length]\n",
    "            \n",
    "            # Apply window\n",
    "            windowed = frame * np.hanning(len(frame))\n",
    "            \n",
    "            # Autocorrelation\n",
    "            autocorr = np.correlate(windowed, windowed, mode='full')\n",
    "            autocorr = autocorr[len(autocorr)//2:]\n",
    "            \n",
    "            # Find peaks\n",
    "            min_period = int(sr / 500)  # 500 Hz max\n",
    "            max_period = int(sr / 50)   # 50 Hz min\n",
    "            \n",
    "            if max_period < len(autocorr):\n",
    "                autocorr_subset = autocorr[min_period:max_period]\n",
    "                if len(autocorr_subset) > 0 and np.max(autocorr_subset) > 0:\n",
    "                    peak_idx = np.argmax(autocorr_subset) + min_period\n",
    "                    f0 = sr / peak_idx\n",
    "                    f0_values.append(f0)\n",
    "                    times.append(i / sr)\n",
    "        \n",
    "        if f0_values:\n",
    "            # Plot pitch contour\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.plot(times, f0_values, 'b-', alpha=0.7, linewidth=1)\n",
    "            plt.ylabel('Frequency (Hz)')\n",
    "            plt.title('Pitch Contour (Fundamental Frequency)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.ylim(0, 500)\n",
    "        \n",
    "        # Method 2: Harmonic-to-Noise Ratio (HNR)\n",
    "        # This can reveal synthesis artifacts\n",
    "        hnr_values = []\n",
    "        \n",
    "        for i in range(0, len(audio) - frame_length, hop_length):\n",
    "            frame = audio[i:i + frame_length]\n",
    "            \n",
    "            # Simple HNR estimation\n",
    "            autocorr = np.correlate(frame, frame, mode='full')\n",
    "            autocorr = autocorr[len(autocorr)//2:]\n",
    "            \n",
    "            if len(autocorr) > 1:\n",
    "                # Ratio of maximum autocorrelation to variance\n",
    "                hnr = np.max(autocorr[1:]) / (autocorr[0] + 1e-10)\n",
    "                hnr_db = 10 * np.log10(hnr + 1e-10)\n",
    "                hnr_values.append(np.clip(hnr_db, -20, 40))\n",
    "        \n",
    "        if hnr_values:\n",
    "            # Plot HNR\n",
    "            plt.subplot(2, 1, 2)\n",
    "            time_hnr = np.arange(len(hnr_values)) * hop_length / sr\n",
    "            plt.plot(time_hnr, hnr_values, 'g-', alpha=0.7, linewidth=1)\n",
    "            plt.ylabel('HNR (dB)')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.title('Harmonic-to-Noise Ratio (Higher values = more harmonic/periodic)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.ylim(-10, 30)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        if f0_values:\n",
    "            print(f\"Pitch Statistics:\")\n",
    "            print(f\"  Mean F0: {np.mean(f0_values):.2f} Hz\")\n",
    "            print(f\"  F0 Std Dev: {np.std(f0_values):.2f} Hz\")\n",
    "        \n",
    "        if hnr_values:\n",
    "            print(f\"\\nHNR Statistics:\")\n",
    "            print(f\"  Mean HNR: {np.mean(hnr_values):.2f} dB\")\n",
    "            print(f\"  HNR Std Dev: {np.std(hnr_values):.2f} dB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Pitch analysis error: {e}\")\n",
    "        print(\"Continuing with other analyses...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346fd1a2-a15d-4a47-a5fc-3463b390ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform advanced analysis\n",
    "phase_diff = analyze_phase_consistency(audio_data, sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74169185-7765-4152-89af-a6f9d7b781dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_formant_consistency(audio_data, sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c4483-18c4-4b60-9476-0d1d5b259ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_pitch_and_harmonics(audio_data, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a259e-2a82-45ad-b94d-901056c386b5",
   "metadata": {},
   "source": [
    "## Pretrained Model Detection\n",
    "This section uses state-of-the-art machine learning models for deepfake detection.\n",
    "\n",
    "### Models Used:\n",
    "\n",
    "MelodyMachine Deepfake Detector (Primary Model - 50% weight):\n",
    "\n",
    "- Specifically trained to detect deepfaked audio\n",
    "- Outputs direct predictions: \"FAKE\", \"REAL\", \"spoof\", or \"bonafide\"\n",
    "- Most reliable indicator with high accuracy on modern TTS\n",
    "\n",
    "\n",
    "### Wav2Vec2 Feature Extraction:\n",
    "\n",
    "- Extracts deep audio representations\n",
    "- Analyzes embedding statistics for anomalies\n",
    "- Helps identify unusual audio patterns\n",
    "\n",
    "\n",
    "### Speech Characteristics Analysis:\n",
    "\n",
    "- Calculates zero-crossing rate and RMS energy patterns\n",
    "- These basic features have shown surprising effectiveness against F5-TTS\n",
    "\n",
    "### Spectral Anomaly Detection:\n",
    "\n",
    "- Identifies unusual spectral patterns\n",
    "- Calculates variance in spectral features\n",
    "- High anomaly scores indicate potential synthesis\n",
    "\n",
    "### How the Model Works:\n",
    "\n",
    "- The deepfake detector uses neural networks trained on thousands of real and fake audio samples\n",
    "- It learned to identify subtle patterns that distinguish synthetic from real speech\n",
    "- The model outputs confidence scores for each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312f5d7-2708-453e-9669-d084b9a9f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_huggingface_models(audio_path):\n",
    "    \"\"\"\n",
    "    Use various Hugging Face models for deepfake detection\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Model 1: Dedicated Deepfake Audio Detection Model\n",
    "    try:\n",
    "        print(\"Loading dedicated deepfake detection model...\")\n",
    "        from transformers import pipeline\n",
    "        \n",
    "        # Use the MelodyMachine deepfake detection model\n",
    "        deepfake_detector = pipeline(\n",
    "            \"audio-classification\",\n",
    "            model=\"MelodyMachine/Deepfake-audio-detection-V2\",\n",
    "            device=-1  # Force CPU\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = deepfake_detector(audio_path)\n",
    "        results['deepfake_detection'] = predictions\n",
    "        \n",
    "        print(\"Deepfake Detection Results:\")\n",
    "        for pred in predictions:\n",
    "            print(f\"  - {pred['label']}: {pred['score']:.4f}\")\n",
    "        \n",
    "        # Extract the fake/real scores\n",
    "        fake_score = 0.0\n",
    "        real_score = 0.0\n",
    "        for pred in predictions:\n",
    "            if 'fake' in pred['label'].lower() or 'spoof' in pred['label'].lower():\n",
    "                fake_score = max(fake_score, pred['score'])\n",
    "            elif 'real' in pred['label'].lower() or 'genuine' in pred['label'].lower() or 'bonafide' in pred['label'].lower():\n",
    "                real_score = max(real_score, pred['score'])\n",
    "        \n",
    "        results['deepfake_classifier_score'] = fake_score\n",
    "        results['real_classifier_score'] = real_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Deepfake detection model error: {e}\")\n",
    "        print(\"Make sure the model is downloaded. This may take a moment on first run.\")\n",
    "    \n",
    "    # Model 2: Feature extraction with Wav2Vec2 (keep as supplementary)\n",
    "    try:\n",
    "        print(\"\\nExtracting supplementary audio features...\")\n",
    "        from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "        \n",
    "        processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        \n",
    "        # Load and process audio\n",
    "        audio_input, sr = librosa.load(audio_path, sr=16000)\n",
    "        inputs = processor(audio_input, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        # Get the last hidden states\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Compute statistics on embeddings\n",
    "        embedding_mean = hidden_states.mean().item()\n",
    "        embedding_std = hidden_states.std().item()\n",
    "        embedding_max = hidden_states.max().item()\n",
    "        embedding_min = hidden_states.min().item()\n",
    "        \n",
    "        results['embedding_stats'] = {\n",
    "            'mean': embedding_mean,\n",
    "            'std': embedding_std,\n",
    "            'max': embedding_max,\n",
    "            'min': embedding_min,\n",
    "            'range': embedding_max - embedding_min\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Feature extraction error: {e}\")\n",
    "    \n",
    "    # Model 3: Speech characteristics analysis\n",
    "    try:\n",
    "        print(\"\\nAnalyzing speech patterns...\")\n",
    "        audio_data, sr = librosa.load(audio_path, sr=16000)\n",
    "        \n",
    "        # Compute speech likelihood using zero-crossing rate and energy\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio_data)[0]\n",
    "        rms = librosa.feature.rms(y=audio_data)[0]\n",
    "        \n",
    "        # Speech typically has specific ZCR and energy patterns\n",
    "        speech_score = np.mean(zcr) * np.mean(rms) * 100\n",
    "        \n",
    "        results['speech_characteristics'] = {\n",
    "            'zcr_mean': float(np.mean(zcr)),\n",
    "            'rms_mean': float(np.mean(rms)),\n",
    "            'speech_likelihood_score': float(speech_score)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Speech analysis error: {e}\")\n",
    "    \n",
    "    # Model 4: Spectral analysis for deepfake artifacts\n",
    "    try:\n",
    "        print(\"\\nPerforming spectral anomaly detection...\")\n",
    "        audio_data, sr = librosa.load(audio_path, sr=16000)\n",
    "        \n",
    "        # Compute spectral features\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sr)[0]\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sr)[0]\n",
    "        \n",
    "        # Analyze spectral consistency\n",
    "        centroid_variance = np.var(spectral_centroids)\n",
    "        rolloff_variance = np.var(spectral_rolloff)\n",
    "        \n",
    "        # High variance might indicate artifacts\n",
    "        anomaly_score = (centroid_variance / 1000000) + (rolloff_variance / 10000000)\n",
    "        \n",
    "        results['spectral_anomaly'] = {\n",
    "            'centroid_variance': float(centroid_variance),\n",
    "            'rolloff_variance': float(rolloff_variance),\n",
    "            'anomaly_score': float(anomaly_score)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Spectral analysis error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run detection with pre-trained models\n",
    "model_results = detect_with_huggingface_models(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a5979-bfb5-44c7-b987-1fdd7b8a67c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a8fcd-3f3b-4eb2-8401-86bbef9f07a9",
   "metadata": {},
   "source": [
    "## Ensamble Detection Score\n",
    "Comprehensive Detection Score\n",
    "This section combines all analyses into a final deepfake detection score.\n",
    "Scoring System:\n",
    "The system uses weighted averaging of multiple indicators:\n",
    "\n",
    "Deepfake Model (50% weight): Direct prediction from specialized neural network\n",
    "Speech Characteristics (20% weight): Based on calibration showing 3x difference between real/fake\n",
    "Phase Coherence (10% weight): Synthetic speech often too consistent\n",
    "Spectral Features (5% weight): Brightness and rolloff patterns\n",
    "Other Features (15% total): Micro-temporal, anomaly scores, etc.\n",
    "\n",
    "Score Interpretation:\n",
    "\n",
    "0.0 - 0.4: Low risk (likely genuine)\n",
    "0.4 - 0.7: Medium risk (suspicious, needs investigation)\n",
    "0.7 - 1.0: High risk (likely deepfake)\n",
    "\n",
    "Component Analysis:\n",
    "Each feature contributes to the final score. The breakdown helps understand:\n",
    "\n",
    "Which aspects triggered detection\n",
    "Confidence in the result\n",
    "Areas needing manual review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea9bb3-83a2-4dd2-8c48-795fac1449eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_deepfake_score(features, model_results):\n",
    "    \"\"\"\n",
    "    Calculate a comprehensive deepfake detection score based on all analyses\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    weights = []\n",
    "    component_details = {}\n",
    "    \n",
    "    # PRIORITY 1: Dedicated deepfake detection model (heavily weighted)\n",
    "    if 'deepfake_classifier_score' in model_results:\n",
    "        fake_score = model_results['deepfake_classifier_score']\n",
    "        real_score = model_results.get('real_classifier_score', 0)\n",
    "        \n",
    "        # The model gives us direct fake/real probabilities\n",
    "        # We'll use the fake score directly as our primary indicator\n",
    "        scores.append(fake_score)\n",
    "        weights.append(0.5)  # 50% weight - this is our most reliable indicator\n",
    "        component_details['deepfake_model'] = fake_score\n",
    "        \n",
    "        # Also store confidence (how certain the model is)\n",
    "        confidence = max(fake_score, real_score)\n",
    "        component_details['model_confidence'] = confidence\n",
    "    \n",
    "    # 2. Speech characteristics (shown to be discriminative in your data)\n",
    "    if 'speech_characteristics' in model_results:\n",
    "        speech_score = model_results['speech_characteristics']['speech_likelihood_score']\n",
    "        # Your data: Real avg ~0.36, Fake avg ~1.15\n",
    "        if speech_score < 0.5:\n",
    "            score = 0.1  # Likely real\n",
    "        elif speech_score < 0.8:\n",
    "            score = 0.4  # Uncertain\n",
    "        elif speech_score < 1.0:\n",
    "            score = 0.7  # Likely fake\n",
    "        else:\n",
    "            score = 0.85  # Very likely fake\n",
    "            \n",
    "        scores.append(score)\n",
    "        weights.append(0.2)  # 20% weight\n",
    "        component_details['speech_score'] = score\n",
    "    \n",
    "    # 3. Spectral feature analysis\n",
    "    if 'spectral_rolloff_std' in features:\n",
    "        rolloff_std = features['spectral_rolloff_std']\n",
    "        optimal_range = (500, 2000)\n",
    "        \n",
    "        if rolloff_std < optimal_range[0]:\n",
    "            score = 1.0 - (rolloff_std / optimal_range[0])\n",
    "        elif rolloff_std > optimal_range[1]:\n",
    "            score = min((rolloff_std - optimal_range[1]) / optimal_range[1], 1.0)\n",
    "        else:\n",
    "            score = 0.0\n",
    "            \n",
    "        scores.append(score)\n",
    "        weights.append(0.05)\n",
    "        component_details['spectral'] = score\n",
    "    \n",
    "    # 4. Phase coherence (if available)\n",
    "    if 'phase_coherence_mean' in features:\n",
    "        pc_mean = features['phase_coherence_mean']\n",
    "        # F5-TTS often has high phase coherence\n",
    "        if pc_mean > 0.9:\n",
    "            score = 0.8  # Too coherent\n",
    "        elif pc_mean > 0.7:\n",
    "            score = 0.5\n",
    "        elif pc_mean < 0.2:\n",
    "            score = 0.6  # Too incoherent\n",
    "        else:\n",
    "            score = 0.2  # Normal range\n",
    "            \n",
    "        scores.append(score)\n",
    "        weights.append(0.1)\n",
    "        component_details['phase'] = score\n",
    "    \n",
    "    # 5. Micro-temporal variance\n",
    "    if 'micro_temporal_variance' in features:\n",
    "        mtv = features['micro_temporal_variance']\n",
    "        if mtv < 0.0001:\n",
    "            score = 0.8  # Very low variance - likely synthetic\n",
    "        elif mtv < 0.001:\n",
    "            score = 0.5\n",
    "        else:\n",
    "            score = 0.2  # Normal variance\n",
    "        \n",
    "        scores.append(score)\n",
    "        weights.append(0.05)\n",
    "        component_details['micro_temporal'] = score\n",
    "    \n",
    "    # 6. Spectral anomaly\n",
    "    if 'spectral_anomaly' in model_results:\n",
    "        anomaly = model_results['spectral_anomaly']['anomaly_score']\n",
    "        if anomaly < 1.0:\n",
    "            score = 0.3\n",
    "        elif anomaly < 1.5:\n",
    "            score = 0.5\n",
    "        else:\n",
    "            score = 0.7\n",
    "            \n",
    "        scores.append(score)\n",
    "        weights.append(0.1)\n",
    "        component_details['anomaly'] = score\n",
    "    \n",
    "    # Calculate weighted average\n",
    "    if scores and weights:\n",
    "        total_weight = sum(weights)\n",
    "        weighted_score = sum(s * w for s, w in zip(scores, weights)) / total_weight\n",
    "    else:\n",
    "        weighted_score = 0.5  # Neutral score if no features available\n",
    "    \n",
    "    return weighted_score, component_details\n",
    "\n",
    "# Calculate final score\n",
    "deepfake_score, component_scores = calculate_deepfake_score(all_features, model_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05d892-1d3e-470b-b071-5223f1432637",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "The output includes three key visualizations:\n",
    "\n",
    "Overall Score Bar: Color-coded risk level with threshold markers\n",
    "Model Predictions: Shows what the neural network detected\n",
    "Component Breakdown: Individual feature contributions\n",
    "\n",
    "This multi-level view helps you:\n",
    "\n",
    "See the final verdict at a glance\n",
    "Understand which features triggered detection\n",
    "Verify the model's confidence\n",
    "Identify areas for manual review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b456cc-7e3e-4b13-b223-0c64c3ad94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Score gauge with model confidence\n",
    "ax1 = plt.subplot(3, 1, 1)\n",
    "score_color = 'red' if deepfake_score > 0.7 else 'orange' if deepfake_score > 0.4 else 'green'\n",
    "ax1.barh(['Detection Score'], [deepfake_score], color=score_color, alpha=0.7)\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_xlabel('Deepfake Likelihood')\n",
    "ax1.set_title(f'Overall Deepfake Detection Score: {deepfake_score:.3f}', fontsize=16)\n",
    "\n",
    "# Add threshold lines\n",
    "ax1.axvline(x=0.4, color='orange', linestyle='--', alpha=0.5, label='Suspicious')\n",
    "ax1.axvline(x=0.7, color='red', linestyle='--', alpha=0.5, label='Likely Deepfake')\n",
    "ax1.legend()\n",
    "\n",
    "# Model predictions if available\n",
    "ax2 = plt.subplot(3, 1, 2)\n",
    "if 'deepfake_detection' in model_results:\n",
    "    predictions = model_results['deepfake_detection']\n",
    "    labels = [p['label'] for p in predictions]\n",
    "    scores = [p['score'] for p in predictions]\n",
    "    colors = ['red' if 'fake' in l.lower() or 'spoof' in l.lower() else 'green' for l in labels]\n",
    "    \n",
    "    bars = ax2.barh(labels, scores, color=colors, alpha=0.7)\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_xlabel('Confidence')\n",
    "    ax2.set_title('Deepfake Detection Model Predictions')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Highlight the winning prediction\n",
    "    max_idx = np.argmax(scores)\n",
    "    ax2.barh(labels[max_idx], scores[max_idx], color=colors[max_idx], alpha=1.0, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Component scores\n",
    "ax3 = plt.subplot(3, 1, 3)\n",
    "if component_scores:\n",
    "    components = list(component_scores.keys())\n",
    "    values = list(component_scores.values())\n",
    "    \n",
    "    bars = ax3.bar(components, values, alpha=0.7)\n",
    "    \n",
    "    # Color bars based on score\n",
    "    for bar, value in zip(bars, values):\n",
    "        if value > 0.7:\n",
    "            bar.set_color('red')\n",
    "        elif value > 0.4:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('green')\n",
    "    \n",
    "    # Highlight the deepfake model component if present\n",
    "    if 'deepfake_model' in components:\n",
    "        idx = components.index('deepfake_model')\n",
    "        bars[idx].set_edgecolor('black')\n",
    "        bars[idx].set_linewidth(2)\n",
    "    \n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.set_title('Component Analysis Scores')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEEPFAKE DETECTION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nOverall Deepfake Likelihood: {deepfake_score:.1%}\")\n",
    "\n",
    "if deepfake_score > 0.7:\n",
    "    print(\"âš ï¸  HIGH RISK: This audio shows strong indicators of being deepfaked\")\n",
    "elif deepfake_score > 0.4:\n",
    "    print(\"âš ï¸  MEDIUM RISK: This audio shows some suspicious characteristics\")\n",
    "else:\n",
    "    print(\"âœ… LOW RISK: This audio appears to be genuine\")\n",
    "\n",
    "# Show model predictions if available\n",
    "if 'deepfake_detection' in model_results:\n",
    "    print(\"\\nðŸ¤– Dedicated Deepfake Model Predictions:\")\n",
    "    for pred in model_results['deepfake_detection']:\n",
    "        print(f\"  - {pred['label']}: {pred['score']:.1%}\")\n",
    "\n",
    "print(\"\\nDetailed Analysis:\")\n",
    "for component, score in component_scores.items():\n",
    "    if component == 'deepfake_model':\n",
    "        print(f\"  - {component.replace('_', ' ').title()} : {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"  - {component.replace('_', ' ').title()}: {score:.3f}\")\n",
    "\n",
    "if 'model_confidence' in component_scores:\n",
    "    print(f\"\\nModel Confidence: {component_scores['model_confidence']:.1%}\")\n",
    "\n",
    "if 'transcription' in model_results:\n",
    "    print(f\"\\nTranscribed Text: {model_results['transcription']['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94f174-3093-4e3b-85ad-6142d9934ffa",
   "metadata": {},
   "source": [
    "## Export Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92130d47-a1a5-4a8d-8082-088c7c27523f",
   "metadata": {},
   "source": [
    "Export Results\n",
    "This section saves the analysis results for documentation and further review.\n",
    "What Gets Exported:\n",
    "\n",
    "Overall deepfake score and risk level\n",
    "All component scores\n",
    "Raw feature values\n",
    "Model predictions\n",
    "Timestamp and file information\n",
    "\n",
    "Format:\n",
    "Results are saved as JSON for easy parsing and integration with other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d9ef59-747b-4d40-9bbc-5a897bd26082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_report(audio_path, features, model_results, deepfake_score, component_scores):\n",
    "    \"\"\"\n",
    "    Export comprehensive analysis report\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        'audio_file': audio_path,\n",
    "        'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'overall_deepfake_score': deepfake_score,\n",
    "        'risk_level': 'HIGH' if deepfake_score > 0.7 else 'MEDIUM' if deepfake_score > 0.4 else 'LOW',\n",
    "        'component_scores': component_scores,\n",
    "        'audio_features': features,\n",
    "        'model_predictions': model_results\n",
    "    }\n",
    "    \n",
    "    # Save as JSON\n",
    "    import json\n",
    "    report_path = audio_path.replace('.wav', '_deepfake_analysis.json')\n",
    "    \n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        return obj\n",
    "    \n",
    "    serializable_report = convert_to_serializable(report)\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(serializable_report, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nAnalysis report saved to: {report_path}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Export the analysis\n",
    "report = export_analysis_report(audio_file_path, all_features, model_results, deepfake_score, component_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
