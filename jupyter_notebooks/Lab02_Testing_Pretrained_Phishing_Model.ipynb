{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-badge",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/RiverGumSecurity/AILabs/blob/main/Lab02_Testing_Pretrained_Phishing_Model.ipynb\" target=\"_new\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-section",
   "metadata": {},
   "source": [
    "# Lab 02: Testing a Pre-trained Phishing Detection Model\n",
    "\n",
    "In this lab, we'll evaluate a pre-trained AI model that detects phishing emails. This demonstrates a key concept in modern AI: **transfer learning** - using models that have already been trained on large datasets for specific tasks.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Load and use a pre-trained BERT model from Hugging Face\n",
    "2. Test the model against real phishing email samples\n",
    "3. Evaluate how well the model performs at classification\n",
    "\n",
    "## About the Model\n",
    "\n",
    "We'll use the `ealvaradob/bert-finetuned-phishing` model from Hugging Face. This model:\n",
    "- Is based on Google's BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- Was fine-tuned specifically for phishing email detection\n",
    "- Classifies text as either **\"phishing\"** or **\"benign\"**\n",
    "\n",
    "**Claimed Performance:**\n",
    "- Accuracy: 97.17%\n",
    "- Precision: 96.58%\n",
    "- Recall: 96.70%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": "## Part 1: Environment Check\n\nThe required packages are pre-installed in this environment. Run this cell to verify everything is ready."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": "# Verify required packages are available\nimport importlib\n\npackages = ['transformers', 'torch', 'pandas']\nmissing = []\n\nfor pkg in packages:\n    try:\n        importlib.import_module(pkg)\n        print(f'[+] {pkg} is available')\n    except ImportError:\n        missing.append(pkg)\n        print(f'[!] {pkg} is missing')\n\nif missing:\n    print(f'\\n[!] Installing missing packages: {\", \".join(missing)}')\n    import subprocess\n    subprocess.check_call(['pip', 'install', '-q'] + missing)\n    print('[+] Installation complete!')\nelse:\n    print('\\n[+] All required packages are available!')"
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "## Part 2: Import Libraries and Load the Model\n",
    "\n",
    "Now we'll import our libraries and download the pre-trained phishing detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-and-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Detect available hardware (GPU speeds up processing significantly)\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print('[+] NVIDIA GPU detected - using CUDA')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print('[+] Apple Silicon GPU detected - using MPS')\n",
    "else:\n",
    "    print('[*] No GPU detected - using CPU (this will be slower)')\n",
    "\n",
    "# Load the pre-trained phishing detection model\n",
    "print('\\n[*] Loading pre-trained phishing detection model...')\n",
    "model_name = \"ealvaradob/bert-finetuned-phishing\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create a prediction pipeline - this simplifies making predictions\n",
    "classifier = transformers.pipeline(\n",
    "    'text-classification', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device=device,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print('[+] Model loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-section",
   "metadata": {},
   "source": [
    "## Part 3: Load the Phishing Email Dataset\n",
    "\n",
    "We'll use the same Kaggle phishing email dataset from Lab 01. This dataset contains real examples of phishing and legitimate (safe) emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Load the phishing email dataset\n# Check for local copy first (pre-downloaded in Docker), otherwise download from URL\nlocal_path = '/home/jovyan/datasets/Phishing_Email.csv.gz'\nremote_url = 'https://raw.githubusercontent.com/RiverGumSecurity/Datasets/refs/heads/main/Kaggle/Phishing_Email.csv.gz'\n\nprint('[*] Loading phishing email dataset...')\n\nif os.path.exists(local_path):\n    print(f'[+] Using pre-downloaded dataset from {local_path}')\n    df = pd.read_csv(local_path)\nelse:\n    print(f'[*] Downloading dataset from remote URL...')\n    df = pd.read_csv(remote_url)\n\n# Clean up the data\ndf = df.drop(['Unnamed: 0'], axis=1, errors='ignore')  # Remove index column if present\ndf = df.dropna()  # Remove rows with missing values\ndf = df.drop_duplicates()  # Remove duplicate entries\n\n# Show dataset statistics\nprint(f'[+] Dataset loaded: {len(df)} emails')\nprint(f'\\nEmail Type Distribution:')\nprint(df['Email Type'].value_counts())"
  },
  {
   "cell_type": "markdown",
   "id": "sample-section",
   "metadata": {},
   "source": [
    "## Part 4: Select Random Test Samples\n",
    "\n",
    "Let's randomly select 10 emails to test the model - 5 phishing emails and 5 safe emails. This gives us a balanced test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 random phishing emails and 5 random safe emails\n",
    "phishing_samples = df[df['Email Type'] == 'Phishing Email'].sample(n=5, random_state=42)\n",
    "safe_samples = df[df['Email Type'] == 'Safe Email'].sample(n=5, random_state=42)\n",
    "\n",
    "# Combine into our test set\n",
    "test_samples = pd.concat([phishing_samples, safe_samples]).reset_index(drop=True)\n",
    "\n",
    "# Shuffle the samples so they're not in order\n",
    "test_samples = test_samples.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "print(f'Selected {len(test_samples)} test emails:')\n",
    "print(f'  - Phishing: {len(test_samples[test_samples[\"Email Type\"] == \"Phishing Email\"])}')\n",
    "print(f'  - Safe: {len(test_samples[test_samples[\"Email Type\"] == \"Safe Email\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preview-section",
   "metadata": {},
   "source": [
    "## Part 5: Preview the Test Emails\n",
    "\n",
    "Let's look at snippets of our test emails to understand what the model will be analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preview-emails",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a preview of each test email\n",
    "print('=' * 80)\n",
    "print('TEST EMAIL PREVIEWS')\n",
    "print('=' * 80)\n",
    "\n",
    "for idx, row in test_samples.iterrows():\n",
    "    email_text = row['Email Text']\n",
    "    actual_label = row['Email Type']\n",
    "    \n",
    "    # Show first 200 characters of each email\n",
    "    preview = email_text[:200].replace('\\n', ' ') + '...' if len(email_text) > 200 else email_text.replace('\\n', ' ')\n",
    "    \n",
    "    print(f'\\nEmail #{idx + 1} [Actual: {actual_label}]')\n",
    "    print('-' * 60)\n",
    "    print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing-section",
   "metadata": {},
   "source": [
    "## Part 6: Run the Model and Evaluate Results\n",
    "\n",
    "Now let's run each email through the BERT model and see how well it classifies them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on all test emails\n",
    "print('[*] Running phishing detection model on test emails...\\n')\n",
    "\n",
    "results = []\n",
    "correct = 0\n",
    "total = len(test_samples)\n",
    "\n",
    "for idx, row in test_samples.iterrows():\n",
    "    email_text = row['Email Text']\n",
    "    actual_label = row['Email Type']\n",
    "    \n",
    "    # Get model prediction\n",
    "    prediction = classifier(email_text)[0]\n",
    "    predicted_label = prediction['label']  # 'phishing' or 'benign'\n",
    "    confidence = prediction['score']\n",
    "    \n",
    "    # Convert actual label to match model output format\n",
    "    actual_simple = 'phishing' if actual_label == 'Phishing Email' else 'benign'\n",
    "    \n",
    "    # Check if prediction is correct\n",
    "    is_correct = (predicted_label == actual_simple)\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    \n",
    "    # Store result\n",
    "    results.append({\n",
    "        'Email #': idx + 1,\n",
    "        'Actual': actual_simple,\n",
    "        'Predicted': predicted_label,\n",
    "        'Confidence': f'{confidence:.1%}',\n",
    "        'Correct': '✓' if is_correct else '✗'\n",
    "    })\n",
    "\n",
    "# Display results as a table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Show accuracy\n",
    "accuracy = correct / total * 100\n",
    "print(f'\\n{\"=\" * 50}')\n",
    "print(f'RESULTS: {correct}/{total} correct ({accuracy:.0f}% accuracy)')\n",
    "print(f'{\"=\" * 50}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-section",
   "metadata": {},
   "source": [
    "## Part 7: Detailed Analysis\n",
    "\n",
    "Let's look more closely at any emails the model got wrong (if any) and understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified emails\n",
    "misclassified = [r for r in results if r['Correct'] == '✗']\n",
    "\n",
    "if misclassified:\n",
    "    print(f'The model misclassified {len(misclassified)} email(s):\\n')\n",
    "    \n",
    "    for miss in misclassified:\n",
    "        email_num = miss['Email #']\n",
    "        email_text = test_samples.iloc[email_num - 1]['Email Text']\n",
    "        \n",
    "        print(f'Email #{email_num}:')\n",
    "        print(f'  Actual: {miss[\"Actual\"]}')\n",
    "        print(f'  Predicted: {miss[\"Predicted\"]} ({miss[\"Confidence\"]} confidence)')\n",
    "        print(f'  Content preview: {email_text[:300]}...\\n')\n",
    "else:\n",
    "    print('The model correctly classified all test emails!')\n",
    "    print('\\nThis aligns with the model\\'s claimed 97% accuracy rate.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-section",
   "metadata": {},
   "source": [
    "## Part 8: Try Your Own Text\n",
    "\n",
    "Now it's your turn! Modify the text below and run the cell to test the model with your own examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "try-your-own",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own email text!\n",
    "# Replace the text below with any email content you want to test\n",
    "\n",
    "test_text = \"\"\"\n",
    "URGENT: Your account has been compromised!\n",
    "\n",
    "Dear valued customer,\n",
    "\n",
    "We have detected suspicious activity on your account. \n",
    "Please click the link below immediately to verify your identity \n",
    "and prevent unauthorized access:\n",
    "\n",
    "http://secure-verify-account.com/login\n",
    "\n",
    "If you do not verify within 24 hours, your account will be suspended.\n",
    "\n",
    "Best regards,\n",
    "Security Team\n",
    "\"\"\"\n",
    "\n",
    "# Run the model\n",
    "result = classifier(test_text)[0]\n",
    "\n",
    "print('Your Test Results:')\n",
    "print('=' * 40)\n",
    "print(f'Classification: {result[\"label\"].upper()}')\n",
    "print(f'Confidence: {result[\"score\"]:.1%}')\n",
    "print('=' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, we:\n",
    "\n",
    "1. **Loaded a pre-trained BERT model** specifically fine-tuned for phishing detection\n",
    "2. **Tested it against real emails** from a labeled dataset\n",
    "3. **Evaluated its performance** and saw how well it distinguishes phishing from legitimate emails\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Transfer learning** allows us to use powerful models without training from scratch\n",
    "- Pre-trained models can achieve high accuracy on specific tasks\n",
    "- Even high-accuracy models can make mistakes - no model is perfect\n",
    "- The model's confidence score indicates how certain it is about each prediction\n",
    "\n",
    "### Discussion Questions\n",
    "\n",
    "1. What types of emails might be hardest for the model to classify correctly?\n",
    "2. How might an attacker try to craft phishing emails that evade this detection?\n",
    "3. Should we trust a 97% accuracy rate for critical security decisions?\n",
    "4. What are the risks of false positives (legitimate emails marked as phishing) vs false negatives (phishing emails marked as safe)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}